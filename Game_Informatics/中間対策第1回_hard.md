**問題 1**
機械学習の 3 つの主要なカテゴリに関する記述のうち，**最も不適切**なものを選んでください．

1 つ選択してください:

1. 教師あり学習は，入力とそれに対応する「正解」のペアを用いてモデルを学習し，未知の入力に対する出力を予測する．主にクラス分類と回帰に大別される．
2. 教師なし学習は，正解ラベルなしのデータからパターンや構造（例：クラスタリング）を見つけ出すことを目的とする．スライドのピッチャーの球種分析(p.26)は，この一種と解釈できる可能性がある．
3. 強化学習は，エージェントが環境内で試行錯誤し，得られる報酬を最大化するように行動方策を学習する．学習データとして明確な「正解」行動が事前に全て与えられるわけではない．
4. ある EC サイトが顧客の購買履歴（説明変数）と特定商品の購入有無（目的変数）のデータを用いて，購入予測モデルを構築するのは，入力データ間の関係性に着目する教師なし学習の典型例である．

<details>
<summary>解答解説を表示</summary>
4

**解説:**
1, 2, 3 はそれぞれの学習カテゴリの特徴を概ね正しく述べています．特に 2 の球種分析は，ラベルなしで球種のグループを見つけるタスクと解釈すれば教師なし学習（クラスタリング）の例と見なせます．
4 は**不適切**です．「説明変数」と「目的変数（正解ラベル）」を用いて予測モデルを構築するのは**教師あり学習**の典型例です．入力データ間の関係性（だけ）に着目するのは教師なし学習です．

</details>

---

**問題 2**
教師あり学習における「入力空間」と「出力空間」の概念について，**最も適切**な記述を選んでください．

1 つ選択してください:

1. 入力空間は常に実数値ベクトルで表現され，出力空間は常に離散的なクラスラベルでなければならない．
2. チョコの値段予測(p.7)では入力空間が個数（離散値），出力空間が値段（実数値）であり，不快度予測(p.8)では入力空間が温度・湿度（実数値），出力空間が Yes/No（離散値）である．
3. k-NN 法を用いる場合，入力空間の次元数が多いほど（例：高解像度画像），一般的に予測精度は向上する傾向にある．
4. 教師あり学習の目標は，入力空間から出力空間への写像を学習することであり，この写像は常に線形関数でなければならない．

<details>
<summary>解答解説を表示</summary>
 2

**解説:**

1. 不適切です．入力も出力も実数値（回帰）や離散値（分類）など様々な組み合わせがありえます．
2. **適切**です．スライドの例(p.13)に基づいています．入力・出力は問題設定に応じて様々な型を取りえます．
3. 不適切です．k-NN は高次元データ（次元の呪い）ではうまく機能しないことが多いとされています．
4. 不適切です．学習する写像（モデル）は線形とは限りません．非線形な関係性を学習する必要がある場合も多くあります（例：線形分離不可能な問題）．

</details>

---

**問題 3**
Lazy Learning（事例ベース学習）と Eager Learning（入出力モデルベース学習）に関する記述として，**最も適切**なものを選んでください．

1 つ選択してください:

1. 1-NN や k-NN は Eager Learning の代表例であり，学習段階で複雑なモデルを構築するため，学習に時間がかかる．
2. Eager Learning は学習データをすべて記憶しておき，予測時にそれらを参照して計算するため，予測に時間がかかるが，学習は瞬時に終わる．
3. Lazy Learning は予測時に学習データとの距離計算などを行うため，データ量が多いと予測に時間がかかる可能性があるが，事前に複雑なモデルを構築しない．
4. Lazy Learning は学習データ全体から汎化されたモデルを構築するため，ノイズに対して常に頑健である．

<details>
<summary>解答解説を表示</summary>
 3

**解説:**

1. 不適切です．1-NN や k-NN は Lazy Learning の代表例です．学習（データ保持）は高速ですが，予測に時間がかかることがあります．
2. 不適切です．これは Lazy Learning の特徴（特にデメリット）を説明しています．Eager Learning は学習に時間をかけモデルを構築し，予測は比較的速い傾向があります．
3. **適切**です．Lazy Learning（例：k-NN）は学習フェーズがほぼなく，予測時に計算が集中します．そのためデータ量が多いと予測が遅くなるデメリットがあります．
4. 不適切です．Lazy Learning の代表である 1-NN はノイズに非常に弱いことが知られています．k-NN である程度頑健性を高められますが，「常に頑健」ではありません．

</details>

---

**問題 4**
k-Nearest Neighbor (k-NN) 法におけるハイパーパラメータ $k$ の選択について，**最も不適切**な記述を選んでください．

1 つ選択してください:

1. $k$ を大きくすると，単一のノイズデータの影響を受けにくくなり，決定境界はより滑らかになる傾向がある．
2. $k$ を 1 に設定（1-NN）すると，学習データに対する分類精度は 100%に近くなるが，未知データに対する汎化性能が最適とは限らない．
3. 2 値分類問題においては，$k$ を偶数に設定すると，近傍点のクラスが同数になった場合に分類を決定できなくなる可能性があるため，奇数に設定することが推奨される場合がある．
4. $k$ の値を学習データの総数$N$に近づけていくと，モデルはより複雑になり，学習データに過剰適合（オーバーフィッティング）しやすくなる．

<details>
<summary>解答解説を表示</summary>
 4

**解説:**
1, 2, 3 は k-NN における$k$の役割や選択に関する一般的な知見であり，適切です．
4 は**不適切**です．$k$ を学習データ総数$N$に近づけると，全てのデータを参照することになり，予測結果は常に学習データ全体の最頻クラス（分類の場合）や平均値（回帰の場合）に近づきます．これは非常に単純化されたモデルであり，過剰適合とは逆のアンダーフィッティング（学習不足）の状態になります．$k$が小さい（特に$k=1$）方が過剰適合しやすいです．

</details>

---

**問題 5**
データが持つ困難さの一つである「線形分離不可能性」について，**最も適切**な記述を選んでください．

1 つ選択してください:

1. 線形分離不可能なデータセットは，どのような非線形な変換を施しても，決して線形分離可能な空間に写像することはできない．
2. スライド p.19 の例のようなデータセットは，単一の直線（または平面，超平面）ではクラスを完全に分離できないことを意味する．
3. 線形分離不可能なデータに対して k-NN 法を適用することは原理的に不可能である．
4. あるデータセットが線形分離不可能である場合，そのデータセットには必ずノイズが含まれている．

<details>
<summary>解答解説を表示</summary>
 2

**解説:**

1. 不適切です．特徴量空間をより高次元に写像するなどの非線形変換により，線形分離可能になる場合は多くあります（例：カーネル法）．
2. **適切**です．これが線形分離不可能性の定義です．
3. 不適切です．k-NN 法は決定境界が線形である必要はないため，線形分離不可能なデータに対しても適用可能であり，むしろ得意とする場合があります．
4. 不適切です．線形分離不可能であることとノイズの存在は必ずしも連動しません．ノイズがなくても線形分離不可能な配置はありえます（例：XOR 問題）．

</details>

---

**問題 6**
データが持つ困難さの一つである「イルスケール性」とその対処法について，**最も不適切**な記述を選んでください．

1 つ選択してください:

1. 各入力変数（特徴量）の値の範囲（スケール）が大きく異なると，ユークリッド距離に基づく手法（例：k-NN）ではスケールの大きい変数の影響が支配的になる可能性がある．
2. 対処法の一つとして，各変数を平均 0，標準偏差 1 に変換する「標準化」や，最小値 0，最大値 1 に変換する「正規化（最小最大スケーリング）」がある．
3. データの正規化や標準化を行えば，イルスケール性の問題は完全に解決され，常にモデルの性能が向上することが保証される．
4. イルスケール性は，変数間の相対的な重要度を歪める可能性があるため，距離計算を伴わない決定木のような手法では影響が比較的小さい場合がある．

<details>
<summary>解答解説を表示</summary>
 3

**解説:**
1, 2 はイルスケール性の問題とその代表的な対処法を正しく説明しています．
4 も概ね正しい考察です．決定木は各ステップで単一の特徴量とその閾値で分割するため，変数間のスケール差の影響を受けにくいとされます．
3 は**不適切**です．正規化や標準化は有効な対処法ですが，「完全に解決」や「常に性能が向上」「保証される」といった強い断定は誤りです．データやモデルによっては効果が薄い場合や，他の問題（例：外れ値の影響増大）を引き起こす可能性もあります．

</details>

---

**問題 7**
学習データに含まれる「ノイズ」について，**最も適切**な記述を選んでください．

1 つ選択してください:

1. ノイズとは，学習データにおいて複数の入力変数が互いに強い相関を持つ状態（変数間依存性）を指す．
2. 1-NN 法は，最も近い 1 点のデータのみを参照するため，その 1 点がノイズであった場合の影響を直接受ける可能性が高い．
3. 機械学習モデルの性能向上のためには，学習データからノイズを完全に除去することが常に最善の方法である．
4. ノイズが多いデータセットに対しては，モデルの複雑度をできるだけ高く設定し，ノイズを含む全てのデータ点を正確に学習させることが望ましい．

<details>
<summary>解答解説を表示</summary>
 2

**解説:**

1. 不適切です．変数間の強い相関は「変数間依存性」の問題です．ノイズは例外的なデータ点や誤測定などを指します．
2. **適切**です．1-NN はノイズに対して脆弱であるとされています．k-NN (k>1) で多数決を取ることで，この影響を緩和できます．
3. 不適切です．ノイズの中には，単なる誤りではなく稀な（しかし正しい）ケースを表すデータも含まれる可能性があります．ノイズを安易に除去すると，そのような稀なケースに対応できなくなる可能性があります．また，どこまでがノイズかの判断も難しい場合があります．
4. 不適切です．ノイズが多いデータに対してモデルの複雑度を高くしすぎると，ノイズまで学習してしまい，未知データに対する性能（汎化性能）が著しく低下する「過学習」を引き起こします．

</details>

---

**問題 8**
教師あり学習の応用例とその際の注意点について，**最も不適切**な組み合わせを選んでください．

1 つ選択してください:

1. スパムメール検出：スパムと非スパムの両方のラベル付きメールデータを集めて学習させる必要がある．スパムメールだけのデータでは効果的な分類器は作れない．
2. 手書き文字認識：様々な人の多様な書き方の文字データ（数字 0 ～ 9 の画像と対応するラベル）を学習データとして用いることで，未知の文字に対する認識精度を高める．
3. 株価予測：過去の株価変動（入力）と未来の株価（出力）の関係を学習する回帰問題として扱えるが，市場の変動要因は複雑であり，過去のデータだけでは未来を正確に予測することは極めて困難である．
4. 医療画像診断：少数の専門医がラベル付けした限られた症例データのみを用いて学習すれば，常に人間と同等以上の精度で未知の症例を診断できるモデルが構築できる．

<details>
<summary>解答解説を表示</summary>
 4

**解説:**
1, 2, 3 はそれぞれの応用例における基本的な考え方や注意点として適切です．
4 は**不適切**です．教師あり学習，特に医療のような重要分野では，高品質なラベル付きデータが大量に必要となることが多いです．「限られたデータ」で「常に人間と同等以上」の精度が「構築できる」と断定するのは現実的ではありません．データの量や質，モデルの選択，評価方法など多くの課題があります．

</details>

---

**問題 9**
k-NN 法に関する以下の記述のうち，**最も適切**なものを選んでください．

1 つ選択してください:

1. k-NN は学習フェーズで入力データと出力の関係性を表す複雑な関数モデルを構築するため，学習には時間がかかるが予測は高速である．
2. k-NN は回帰問題には適用できず，クラス分類問題専用の手法である．
3. k-NN では，予測したい未知データと全ての学習データとの距離を計算する必要があるため，学習データ量が多いと予測時の計算コストが増大する．
4. k-NN は予測結果の根拠（どの近傍データを参照したか）を説明することが困難であり，「ブラックボックス」化しやすいモデルの代表例である．

<details>
<summary>解答解説を表示</summary>
 3

**解説:**

1. 不適切です．これは Eager Learning の説明です．k-NN は Lazy Learning であり，学習は高速（データを覚えるだけ），予測に時間がかかります．
2. 不適切です．k-NN はクラス分類だけでなく，回帰問題にも適用可能です．回帰の場合は，k 個の近傍点の出力値の平均などを予測値とします．
3. **適切**です．k-NN の主なデメリットとして，予測時に全学習データとの距離計算が必要なため，データ量 N に対して計算量が O(N)となり，大規模データセットでは予測が遅くなる点が挙げられます．
4. 不適切です．k-NN は「どの k 個の近傍データに基づいて予測したか」を示せるため，予測結果の解釈が比較的容易であり，ブラックボックス化しにくいモデルとされています．ブラックボックス化が問題視されやすいのは，ニューラルネットワークなど複雑なモデルです．

</details>

---

**問題 10**
教師あり学習における「学習データ」の質と量に関する記述として，**最も不適切**なものを選んでください．

1 つ選択してください:

1. 学習データの量が十分であっても，データに偏り（バイアス）がある場合，構築されるモデルの汎化性能は低くなる可能性がある．
2. 学習データに含まれるノイズや外れ値は，モデルの学習を妨げる可能性があるため，前処理段階で適切に対処することが望ましい．
3. 教師あり学習では，ラベル付けされたデータが大量に必要となることが多く，このラベル付け作業が高コストになることが課題の一つである．
4. 学習データの質を高めるためには，できるだけ特定の条件下（例えば，特定の年齢層や性別のみ）で収集された均一なデータを用いるべきである．

<details>
<summary>解答解説を表示</summary>
 4

**解説:**
1, 2, 3 は学習データの質と量に関する重要な指摘であり，適切です．
4 は**不適切**です．学習データの質を高め，汎用的なモデルを構築するためには，予測したい対象全体の多様性を反映するように，様々な条件下でデータを収集することが重要です．特定の条件下に偏ったデータでは，その条件以外での予測精度（汎化性能）が低下してしまいます．

</details>

---

**問題 11**
以下の機械学習タスクと，それに最も適していると考えられる学習タイプ（教師あり/なし/強化）および具体的な問題設定（分類/回帰/クラスタリングなど）の組み合わせとして，**最も適切**なものを選んでください．

1 つ選択してください:

1. 顧客の Web サイト閲覧履歴やデモグラフィック情報から，その顧客が将来購入する可能性のある商品カテゴリを推薦する → 教師なし学習（回帰）．
2. 大量のニュース記事を自動的に「政治」「経済」「スポーツ」「エンタメ」などのカテゴリに分類する．各カテゴリに属する少数の記事例が与えられている． → 強化学習．
3. センサーデータ（温度，振動など）を監視し，機械が故障する「前兆」を検知する．過去の正常時データと故障前後のデータがある． → 教師あり学習（クラス分類：正常/異常 or 故障前兆）．
4. ロボットアームが，様々な形状や重さの物体を掴んで指定の場所に置く動作を，試行錯誤を通じて学習する．成功・失敗に応じてフィードバックが与えられる． → 教師なし学習（クラスタリング）．

<details>
<summary>解答解説を表示</summary>
 3

**解説:**

1. 不適切です．商品推薦は様々なアプローチがありますが，過去の購入履歴などから予測する場合，教師あり学習（特にランキングや分類）や，協調フィルタリングのような手法が考えられます．回帰は連続値予測であり，カテゴリ推薦には通常用いられません．また，単に閲覧履歴からグループ分けするなら教師なし（クラスタリング）も考えられますが，「購入可能性のある商品カテゴリを推薦」は予測タスクの色合いが強いです．
2. 不適切です．少数のラベル付き事例をもとにカテゴリ分類ルールを学習するのは，教師あり学習（クラス分類）の典型例です．強化学習は通常，行動と報酬の学習に用いられます．
3. **適切**です．過去の正常/異常（または故障前兆）データという「正解ラベル」付きデータを用いて，未知のセンサーデータがどちらに属するかを予測するのは，教師あり学習のクラス分類問題として自然な設定です．
4. 不適切です．試行錯誤とフィードバック（報酬）を通じて最適な行動（掴み方）を学習するのは，強化学習の典型的な応用例です．

</details>

---

**問題 12**
スライド p.18 の「変数間依存性」の例（体重と身長）について，これが機械学習モデルに与える影響として考えられることのうち，**最も不適切**な記述を選んでください．

1 つ選択してください:

1. 体重と身長のように強い相関がある変数を両方とも入力として使うと，一方の変数が持つ情報が他方に含まれてしまい，モデルにとって冗長な情報となる可能性がある．
2. 変数間依存性が強い場合，特定のモデル（例：線形回帰）では係数の推定が不安定になる「多重共線性」と呼ばれる問題を引き起こすことがある．
3. 変数間依存性がある場合，k-NN 法のような距離ベースの手法では，実質的に同じような情報を二重に考慮してしまい，距離計算の解釈を歪める可能性がある．
4. 変数間依存性が存在するデータセットに対しては，教師あり学習を適用すること自体が不可能になるため，必ず変数選択や次元削減によって依存性を解消しなければならない．

<details>
<summary>解答解説を表示</summary>
 4

**解説:**
1, 2, 3 は変数間依存性がモデルに与えうる影響として考えられる適切な内容です．冗長性，多重共線性，距離計算への影響などが指摘されます．
4 は**不適切**です．変数間依存性があっても教師あり学習を適用することは可能です．ただし，上記のような問題を引き起こす可能性があるため，モデルの選択に注意したり，事前に対処（変数選択，次元削減，正則化など）したりすることが望ましい場合はあります．しかし，「不可能になる」「必ず解消しなければならない」というのは言い過ぎです．

</details>

---

**問題 13**
1-NN 法と k-NN 法 (k>1) の性質に関する比較として，**最も適切**な記述を選んでください．

1 つ選択してください:

1. 学習に必要な計算時間は，1-NN よりも k-NN の方が一般的に短い．
2. 予測に必要な計算時間は，学習データ数を N とすると，1-NN も k-NN もほぼ同程度であり，N に依存しない．
3. 1-NN は学習データに完全にフィットするため過学習しやすいが，k-NN で k を適切に選ぶことで，より汎化性能の高い（未知データに強い）モデルを期待できる．
4. 決定境界の滑らかさは，1-NN の方が k-NN よりも滑らかになる傾向がある．

<details>
<summary>解答解説を表示</summary>
 3

**解説:**

1. 不適切です．どちらも Lazy Learning であり，学習フェーズでの計算時間はほぼゼロ（データを覚えるだけ）です．
2. 不適切です．予測時には未知データと全学習データ N 個との距離計算が必要なため，予測時間は N に比例して増加します（単純な実装では O(N)）．k-NN はさらに k 個の近傍点を見つけ多数決などを行うため，1-NN より僅かに計算が増えますが，主要な計算ボトルネックは N 個のデータとの距離計算です．
3. **適切**です．1-NN は学習データに過剰適合しやすく，ノイズの影響も受けやすいです．k を適切に設定した k-NN は，多数決によりノイズの影響を緩和し，より滑らかな決定境界を形成するため，汎化性能の向上が期待できます．
4. 不適切です．1-NN の決定境界は学習データ点の配置に非常に敏感で，ギザギザになりやすいです．k-NN で k を大きくするほど，決定境界は滑らかになる傾向があります．

</details>

---

**問題 14**
教師あり学習において，「学習データ」と構築される「モデル（分類器や回帰器）」の関係について，**最も不適切**な記述を選んでください．

1 つ選択してください:

1. 教師あり学習のプロセスは，「学習データ（入力と正解出力のペア群）」を入力とし，「未知の入力に対して出力を予測するモデル」を出力と見なすことができる．
2. Eager Learning では学習後にモデルが構築されるため，予測時には元の学習データは不要になる場合が多い．一方，Lazy Learning (k-NN) では予測時に学習データそのものを参照する．
3. 構築されたモデルの性能は，学習に使用したデータ（訓練データ）に対する精度だけでなく，学習に使用していない未知のデータ（テストデータ）に対する精度（汎化性能）で評価することが重要である．
4. 学習データが完全にノイズフリーで代表性も高い場合，構築されたモデルは常に 100%の精度で未知のデータを予測できるはずである．

<details>
<summary>解答解説を表示</summary>
 4

**解説:**
1, 2, 3 は教師あり学習における学習データとモデルの関係性，および評価に関する基本的な考え方として適切です．
4 は**不適切**です．たとえ学習データが理想的であったとしても，現実世界の現象は複雑であり，学習データに含まれていない未知のパターンや変動要因が存在する可能性があります．そのため，学習データから構築されたモデルが，未知のデータに対して常に 100%の精度を達成できる保証はありません．学習はあくまでデータからパターンを「推定」するプロセスであり，本質的な限界があります．

</details>

---

**問題 15**
以下の記述のうち，スライドまたは検索結果の内容に照らして，**最も疑わしい**（根拠が薄い，または誤解を招く可能性がある）ものを選んでください．

1 つ選択してください:

1. k-NN 法はアルゴリズムが単純で理解しやすく，結果の解釈もしやすいため，予測結果の「ブラックボックス化」を避けたい場合に有効な選択肢となりうる．
2. 教師あり学習では，説明変数と目的変数の間に実際には因果関係がないにも関わらず，見かけ上の相関（疑似相関）を学習してしまうリスクがあるため，結果の解釈には注意が必要である．
3. 1-NN は学習データに対する正解率がほぼ 100%になるが，これはモデルの性能が良いことを直接意味するわけではなく，むしろ過学習の兆候である可能性がある．
4. 教師あり学習を用いれば，どのような複雑な問題であっても，十分な量のラベル付きデータと計算資源さえあれば，必ず人間を超える精度の予測モデルを構築することが可能である．

<details>
<summary>解答解説を表示</summary>
 4

**解説:**
1, 2, 3 はそれぞれ検索結果,, スライドおよび検索結果で支持される内容であり，妥当な記述です．
4 は**最も疑わしい**記述です．教師あり学習は強力な技術ですが，「どのような問題でも」「十分なデータと計算資源があれば」「必ず人間を超える精度」を達成できるというのは過度に楽観的であり，保証はありません．問題の性質（本質的な予測不可能性など），データの質（バイアス，ノイズ），モデルの限界，評価指標の妥当性など，多くの要因が絡み合います．現状では，特定のタスクにおいては人間を超える性能を示す例もありますが，万能ではありません．

</details>
