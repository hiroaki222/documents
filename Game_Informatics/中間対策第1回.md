**問題 1**

機械学習の主な 3 つの分類に関する記述として，最も適切なものを選んでください．

1 つ選択してください:

1. 教師あり学習では，データ間の関係性のみを学習し，グループ分けを行う．
2. 教師なし学習では，正解ラベル付きのデータを用いて，未知のデータに対する予測モデルを構築する．
3. 強化学習では，行動に対する報酬（点数）をもとに，試行錯誤を通じて最適な行動方針を学習する．
4. 機械学習の分類は主に 2 つであり，教師あり学習と強化学習に大別される．

<details>
<summary>解答解説を表示</summary>
3

**解説:**

1. データ間の関係性を学習しグループ分けを行うのは教師なし学習（クラスタリング）です．
2. 正解ラベル付きデータを用いるのは教師あり学習です．
3. 強化学習は，行動に対する報酬（点数）を指標に試行錯誤によって最適な行動（政策）を学習する手法です．
4. 機械学習は主に，教師あり学習，教師なし学習，強化学習の 3 つに分類されます．
</details>

---

**問題 2**

以下の具体例と機械学習の分類の組み合わせとして，最も適切なものを選んでください．

1 つ選択してください:

1. 手書き文字画像とその文字（数字）のラベルを与えて学習し，未知の手書き文字画像を認識する → 強化学習．
2. 顧客の購買履歴データから，顧客をいくつかのグループに分類する → 教師あり学習．
3. 囲碁の対局を繰り返し行い，より勝率の高くなる着手を選択できるように学習する → 教師なし学習．
4. あるウェブサイトを訪れたユーザーの閲覧履歴や属性情報から，そのユーザーが広告をクリックするかどうかを予測するモデルを構築する → 教師あり学習．

<details>
<summary>解答解説を表示</summary> 4

**解説:**

1. 手書き文字認識のように，正解ラベル付きデータを用いるのは教師あり学習の典型例です．
2. 顧客の購買履歴からグループ分けするのは，正解がないデータからパターンを見つける教師なし学習（クラスタリング）の例です．
3. 囲碁 AI のように試行錯誤で最適な行動を学習するのは強化学習の例です．
4. ユーザーの属性情報（入力）とクリック有無（正解ラベル）を用いて予測モデルを作るのは教師あり学習（クラス分類）です．
</details>

---

**問題 3**

教師あり学習における「学習データ」に関する記述として，最も適切なものを選んでください．

1 つ選択してください:

1. 学習データは，入力データ $x$ のみの集合である．
2. 学習データは，出力データ $y$ のみの集合である．
3. 学習データは，入力 $x$ とそれに対応する正解の出力 $y$ のペア $(x, y)$ の集合である．
4. 学習データは，学習の過程で AI が試行錯誤を通じて収集するデータである．

<details>
<summary>解答解説を表示</summary> 3

**解説:**

1, 2. 教師あり学習では，入力とそれに対応する「正解」の出力が必要です．入力のみを用いるのは教師なし学習や，強化学習の一部シナリオです． 3. 教師あり学習は，入力 $x_i$ と正解出力 $y_i$ のペアの集合 $L=\{(x_i, y_i)\}$ を学習データとして用います． 4. 試行錯誤を通じてデータを収集・学習するのは強化学習の特徴です．

</details>

---

**問題 4**

教師あり学習の主な目的として，最も適切なものを選んでください．

1 つ選択してください:

1. 与えられたデータ群の中から，本質的に異なる性質を持つクラスター（グループ）を発見すること．
2. 試行錯誤を通じて，ある環境下で報酬を最大化するような行動戦略を獲得すること．
3. 学習データに基づいて入力と出力の関係性をモデル化し，未知の入力に対する適切な出力を予測（分類や回帰）すること．
4. データの次元数を削減し，より少ない変数でデータを表現すること．

<details>
<summary>解答解説を表示</summary> 3

**解説:**

1. クラスターを発見するのは教師なし学習の目的の一つです．
2. 報酬を最大化する行動戦略を獲得するのは強化学習の目的です．
3. 教師あり学習は，学習データから入力と出力の関係（モデル）を学習し，未知の入力 $x$ に対して適切な出力 $y$ を予測することを目的とします．
4. 次元削減は教師なし学習の一分野として扱われることもあります．
</details>

---

**問題 5**

「スパムメールの検出」を教師あり学習で行う場合，学習データに関する記述として最も適切なものはどれか．

1 つ選択してください:

1. 大量のメールを集め，それぞれのメールに含まれる単語の頻度などを特徴量として学習させる．正解ラベルは不要である．
2. スパムメールと判断されたメールだけを大量に集めて学習させる．
3. スパムメールと非スパムメールの両方を集め，それぞれに「スパム」「非スパム」という正解ラベルを付与して学習させる．
4. メールを送受信するサーバーのログデータを集めて学習させる．

<details>
<summary>解答解説を表示</summary> 3

**解説:**

1. 教師あり学習には正解ラベルが必要です．ラベルなしで単語頻度などから分類するのは教師なし学習のアプローチに近いです．
2. スパムメール検出は「スパムか否か」を分類する問題であり，両方のクラス（スパム，非スパム）のデータが必要です．スパムだけのデータでは，何がスパムでないかを学習できません．
3. スパムメール（正例）と非スパムメール（負例）にそれぞれの正解ラベルを付けて学習させるのが，教師あり学習によるクラス分類の基本的なアプローチです．
4. サーバーログは不正アクセス検出などには使えますが，メールの内容に基づいたスパム検出には直接的ではありません．
</details>

---

**問題 6**

スライド p.13 の表にある問題例について，入力と出力の関係から分類される「呼び方」の組み合わせとして，**適切でない**ものを選んでください．

1 つ選択してください:

1. 入力：実数 1 つ（個数），出力：実数 1 つ（値段） → 関数近似（または回帰）．
2. 入力：実数 2 つ（温度，湿度），出力：Yes/No（不快か） → クラス分類．
3. 入力：離散値 4 つ（将棋の知識），出力：Yes/No（初心者か） → クラス分類．
4. 入力：画像（2 値たくさん），出力：離散値（動物の種類） → 回帰．

<details>
<summary>解答解説を表示</summary> 4

**解説:**

1, 2, 3. スライド p.13 の表にある通り，入力と出力の種類に応じて，関数近似（回帰）やクラス分類と呼ばれます．これらは適切です． 4. 画像を入力として，動物の種類のような離散的なカテゴリを出力するのは「クラス分類」です．「回帰」は通常，出力が連続値（実数値）の場合に用いられます．

</details>

---

**問題 7**

Lazy learning（事例ベース学習）に関する記述として，最も適切なものを選んでください．

1 つ選択してください:

1. 学習データが与えられた時点で，事前に入力と出力の関係を表すモデル（関数や決定木など）を構築する．
2. Nearest Neighbor 法や k-NN 法が代表例であり，予測時には学習データを直接参照して近傍のデータから結果を推定する．
3. 学習に時間がかかるが，予測は高速に行えるという特徴がある．
4. 主に強化学習で用いられる学習パラダイムである．

<details>
<summary>解答解説を表示</summary> 2

**解説:**

1. 事前にモデルを構築するのは Eager learning（入出力モデルベース学習）の特徴です．Lazy learning では予測時まで本格的な計算を行いません．
2. Lazy learning は，予測要求があった際に，保持している学習データの中から近傍のデータを参照して予測を行います．NN 法や k-NN 法がその代表です．
3. Lazy learning は学習（データ保持）は高速ですが，予測時に全データとの距離計算などが必要になる場合があり，予測に時間がかかることがあります．Eager learning はその逆の傾向があります．
4. Lazy learning は主に教師あり学習（特にクラス分類）で用いられる考え方です．
</details>

---

**問題 8**

Eager learning（入出力モデルベース学習）に関する記述として，最も適切なものを選んでください．

1 つ選択してください:

1. Nearest Neighbor 法や k-NN 法が代表的な手法である．
2. 学習データは予測時まで保持し，予測要求があるたびに全データを参照する．
3. 学習段階でデータから汎用的なモデル（例：決定木，ニューラルネットワーク，回帰式）を構築し，予測時にはそのモデルのみを使用する．
4. 学習データが少なくても，常に高い予測精度を達成できる．

<details>
<summary>解答解説を表示</summary> 3

**解説:**

1. NN 法や k-NN 法は Lazy learning の代表例です．
2. 予測時に学習データを直接参照するのは Lazy learning の特徴です．Eager learning では学習済みのモデルを使います．
3. Eager learning は，学習データを用いて事前に入力から出力を予測するモデルを構築します．予測時にはこの構築済みモデルを利用するため，学習データそのものは不要になる場合があります．
4. 学習データが少ない場合，モデルの汎化性能が低くなる可能性があります．Eager learning が常に高い精度を出すとは限りません．
</details>

---

**問題 9**

Nearest Neighbor (1-NN) 法に関する記述として，最も適切なものを選んでください．

1 つ選択してください:

1. 未知のデータ $x$ を分類する際，学習データの中から最も遠いデータ $x_i$ のラベル $y_i$ を予測結果とする．
2. 学習データにノイズが含まれている場合でも，その影響を受けにくい頑健な手法である．
3. 予測したい点の最も近くにある学習データのクラスを採用するため，実装が比較的容易である．
4. 複数の近傍データを考慮するため，計算コストが高いという欠点がある．

<details>
<summary>解答解説を表示</summary> 3

**解説:**

1. 1-NN 法は，未知のデータ $x$ に「最も近い (Nearest)」学習データ $x_{i^*}$ を見つけ，そのラベル $y_{i^*}$ を $x$ の予測結果とします．
2. 1-NN 法は最も近い 1 点のみを参照するため，その 1 点がノイズ（例外的なデータ）だった場合，誤った予測をしてしまう可能性があり，ノイズに弱いとされます．
3. アルゴリズムは「最も近い点を探してそのクラスを使う」という単純なものであり，実装は容易です．
4. 複数の近傍データを考慮するのは k-NN 法 (k>1) です．1-NN は 1 点のみなので，k-NN に比べれば計算コストは（k 倍）低いです．
</details>

---

**問題 10**

k-Nearest Neighbor (k-NN) 法と 1-Nearest Neighbor (1-NN) 法の比較に関する記述として，最も適切なものを選んでください．

1 つ選択してください:

1. 1-NN は学習データに対する正解率（学習時性能）が 100%になりやすいが，未知データに対する性能（汎化性能）が良いとは限らない．
2. k-NN (k>1) は参照するデータが 1 つだけなので，1-NN よりも計算時間が短い．
3. 1-NN の方が複数のデータを参照するため，ノイズに対して頑健である．
4. k-NN では k の値を大きくすればするほど，常に予測精度が向上する．

<details>
<summary>解答解説を表示</summary> 1

**解説:**

1. 1-NN は学習データ自身に最も近いのはそのデータ自身なので，学習データに対する分類はほぼ間違えません（正解率 100%）．しかし，これは学習データに過剰適合しているだけで，未知のデータに対してうまく機能する保証はありません（汎化性能の問題）．
2. k-NN (k>1) は k 個の近傍点を探し，多数決などを取るため，1-NN よりも計算時間が増加する傾向があります．
3. 複数のデータを参照して多数決などで決める k-NN (k>1) の方が，単一のノイズデータの影響を受けにくく，1-NN より頑健であると言えます．
4. k の値を大きくしすぎると，関係のない遠くのデータまで考慮してしまい，かえって精度が低下することがあります．また，計算コストも増大します．適切な k の値を選ぶことが重要です．
</details>

---

**問題 11**

k-Nearest Neighbor (k-NN) 法におけるパラメータ $k$ の値に関する記述として，最も適切なものを選んでください．

1 つ選択してください:

1. $k$ の値は常に 1 とするのが最も良い性能を示す．
2. $k$ の値を大きくすると，計算コストは減少する傾向がある．
3. $k$ の値を大きくしすぎると，モデルが学習データに過剰適合しやすくなる（過学習）．
4. $k$ の値を適切に設定することで，ノイズデータに対する頑健性を高めることができる．

<details>
<summary>解答解説を表示</summary> 4

**解説:**

1. $k=1$ は 1-NN であり，ノイズに弱いなどの欠点があります．常に最適とは限りません．
2. $k$ の値を大きくすると，参照する近傍点の数が増え，多数決などの処理も必要になるため，計算コストは増加する傾向があります．
3. $k$ の値を大きくすると，より広範囲のデータを参照するため，決定境界が滑らかになり，過学習はむしろ抑制される傾向があります．逆に $k$ が小さい（特に $k=1$）と過学習しやすいです．
4. $k$ を 1 より大きくすることで，単一のノイズデータの影響を緩和し，多数決によってより安定した予測が可能になります．これによりノイズに対する頑健性が向上します．ただし，大きすぎると別の問題が生じます．
</details>

---

**問題 12**

「線形分離不可能」なデータセットに関する記述として，最も適切なものを選んでください．

1 つ選択してください:

1. どのような複雑な曲線を用いても，2 つのクラスを完全に分離することができないデータセットのこと．
2. 2 次元データの場合，1 本の直線では 2 つのクラス（例：○ と ×）を完全に分けることができないデータセットのこと．
3. 全てのデータ点が一直線上に並んでいるようなデータセットのこと．
4. データにノイズが全く含まれていない理想的なデータセットのこと．

<details>
<summary>解答解説を表示</summary> 2

**解説:**

1. 線形分離不可能性は，あくまで「線形」分類器（直線，平面，超平面）での分離可能性を問題にします．非線形な曲線や曲面を用いれば分離できる場合も多いです．
2. 線形分離不可能とは，（2 次元の場合）正例と負例を 1 本の直線で完全に分離できない状態を指します．3 次元なら平面，一般の N 次元なら超平面で分離できない場合を言います．スライド p.19 の例が該当します．
3. データ点が一直線上に並んでいるかは線形分離可能性とは直接関係ありません．
4. ノイズの有無と線形分離可能性は別の概念です．ノイズがあっても線形分離可能な場合も，ノイズがなくても線形分離不可能な場合もあります．
</details>

---

**問題 13**

「変数間依存性」に関する記述として，最も適切なものを選んでください．

1 つ選択してください:

1. 入力変数（特徴量）のスケール（値の範囲）が変数ごとに大きく異なること．
2. ある入力変数の値が，他の入力変数の値によってある程度決まってしまう，または強い相関があること．
3. データの中に，他のデータ点から大きく外れた値（外れ値）が含まれていること．
4. 学習データが，モデルが学習すべき本来の分布から偏っていること．

<details>
<summary>解答解説を表示</summary> 2

**解説:**

1. 変数間でスケールが大きく異なるのは「イルスケール性」の問題です．
2. 変数間依存性とは，入力変数間に従属関係や強い相関が存在する状況を指します．例えば，身長と体重のように，一方がある程度決まると他方もある範囲に限定されるような場合です．スライド p.18 の例が該当します．
3. 外れ値が含まれていることは「ノイズ」の問題と関連します．
4. 学習データの偏りはサンプリングバイアスなどの問題ですが，変数間依存性とは異なります．
</details>

---

**問題 14**

「イルスケール性」の問題とその対処法に関する記述として，最も適切なものを選んでください．

1 つ選択してください:

1. イルスケール性とは，学習データに誤ったラベルが付与されている問題であり，データの再ラベル付けで対処する．
2. イルスケール性とは，直線でクラスを分離できない問題であり，非線形モデルを用いることで対処する．
3. イルスケール性とは，入力変数のスケールが異なり，距離計算に基づく手法（k-NN など）で特定の変数の影響が過大になる問題であり，データの正規化（標準化や最小最大スケーリング）で対処することが多い．
4. イルスケール性とは，無関係な入力変数が多く含まれる問題であり，次元削減を行うことで対処する．

<details>
<summary>解答解説を表示</summary> 3

**解説:**

1. 誤ったラベルはノイズの一種です．イルスケール性は変数のスケールの問題です．
2. 直線で分離できないのは線形分離不可能性の問題です．
3. イルスケール性は，例えば変数 A が 0 ～ 1 の範囲，変数 B が 0 ～ 10000 の範囲をとるような場合，ユークリッド距離などを計算すると変数 B の影響が支配的になってしまう問題です．これを避けるため，各変数を同じ範囲（例：0 ～ 1）や同じ分散（例：平均 0，標準偏差 1）に揃える正規化（Normalization）や標準化（Standardization）が行われます．スライド p.22 の例が該当します．
4. 無関係な変数が含まれるのは次元の呪いや冗長性の問題であり，次元削減や特徴選択で対処します．
</details>

---

**問題 15**

学習データに含まれる「ノイズ」に関する記述として，最も適切なものを選んでください．

1 つ選択してください:

1. ノイズとは，入力変数間のスケールが大きく異なることである．
2. ノイズとは，本来のデータ分布から外れた例外的なデータや，誤って測定・記録されたデータなどのことである．
3. ノイズが多いほど，機械学習モデルの学習は容易になる．
4. 1-NN 法は，k-NN 法 (k>1) よりもノイズに対して頑健である．

<details>
<summary>解答解説を表示</summary> 2

**解説:**

1. 変数間のスケールが異なるのはイルスケール性です．
2. ノイズは，測定誤差，記録ミス，あるいは本質的に例外的（だが正しい）なデータなど，主なデータの傾向から外れたものを広く指します．スライド p.23 の例が該当します．
3. ノイズが多いと，モデルがノイズまで学習しようとしてしまい（過学習），汎化性能が低下するため，学習はより難しくなります．
4. 1-NN は最も近い 1 点のみを見るため，その点がノイズだと予測が大きく影響されます．複数の近傍点を参照する k-NN (k>1) の方が，多数決などによりノイズの影響を緩和できるため，一般にノイズに対してより頑健です．
